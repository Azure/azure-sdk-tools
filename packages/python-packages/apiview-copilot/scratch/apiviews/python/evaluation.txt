# Package is parsed using apiview-stub-generator(version:0.3.18), Python version: 3.10.16


namespace azure.ai.evaluation
	
	def azure.ai.evaluation.evaluate(
		    *, 
		    azure_ai_project: Optional[AzureAIProject] = ..., 
		    data: Union[str, PathLike], 
		    evaluation_name: Optional[str] = ..., 
		    evaluator_config: Optional[Dict[str, EvaluatorConfig]] = ..., 
		    evaluators: Dict[str, Callable], 
		    fail_on_evaluator_errors: bool = False, 
		    output_path: Optional[Union[str, PathLike]] = ..., 
		    target: Optional[Callable] = ..., 
		    **kwargs
		) -> EvaluationResult
	
	
	@experimental
	class azure.ai.evaluation.AIAgentConverter:
		
		def __init__(self, project_client: AIProjectClient)
		
		def convert(
			    self, 
			    thread_id: str, 
			    run_id: str, 
			    exclude_tool_calls_previous_runs: bool = False
			) -> dict
		
		def prepare_evaluation_data(
			    self, 
			    thread_ids: Union[str, List[str]] = Union[str, List[str]], 
			    filename: str = None
			) -> List[dict]
		
		
	class azure.ai.evaluation.AzureAIProject(TypedDict):
		key "project_name": str
		key "resource_group_name": str
		key "subscription_id": str
		
		
	class azure.ai.evaluation.AzureOpenAIModelConfiguration(TypedDict):
		key "azure_deployment": str
		key "azure_endpoint": str
		ivar api_key: NotRequired[str]
		ivar api_version: NotRequired[str]
		ivar type: NotRequired[Literal['azure_openai']]
		
		
	class azure.ai.evaluation.BleuScoreEvaluator(ABC, Generic[T_EvalValue]):
		ivar id = azureml://registries/azureml/models/Bleu-Score-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			)
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		def __init__(
			    self, 
			    *, 
			    threshold = 0.5
			)
		
		
	@experimental
	class azure.ai.evaluation.CodeVulnerabilityEvaluator(RaiServiceEvaluatorBase[Union[str, bool]]):
		ivar id = code_vulnerability
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Dict[str, Union[str, bool]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject
			)
		
		
	class azure.ai.evaluation.CoherenceEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Coherence-Evaluator/versions/4
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: str = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, float], Dict[str, Union[float, Dict[str, List[float]]]]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = 3
			)
		
		
	@experimental
	class azure.ai.evaluation.ContentSafetyEvaluator(MultiEvaluatorBase[Union[str, float]]):
		ivar id = content_safety
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    hate_unfairness_threshold: int = 3, 
			    self_harm_threshold: int = 3, 
			    sexual_threshold: int = 3, 
			    violence_threshold: int = 3, 
			    **kwargs: Any
			)
		
		
	class azure.ai.evaluation.Conversation(TypedDict):
		key "messages": Union[List[Message], List[Dict]]
		ivar context: NotRequired[Dict[str, Any]]
		
		
	class azure.ai.evaluation.EvaluationResult(TypedDict):
		key "metrics": Dict
		key "rows": List[Dict]
		ivar studio_url: NotRequired[str]
		
		
	class azure.ai.evaluation.EvaluatorConfig(TypedDict):
		key "column_mapping": Dict[str, str]
		
		
	class azure.ai.evaluation.F1ScoreEvaluator(ABC, Generic[T_EvalValue]):
		ivar id = azureml://registries/azureml/models/F1Score-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			) -> Dict[str, float]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		def __init__(
			    self, 
			    *, 
			    threshold = 0.5
			)
		
		
	class azure.ai.evaluation.FluencyEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Fluency-Evaluator/versions/4
		
		@overload
		def __call__(
			    self, 
			    *, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, float], Dict[str, Union[float, Dict[str, List[float]]]]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = 3
			)
		
		
	class azure.ai.evaluation.GleuScoreEvaluator(ABC, Generic[T_EvalValue]):
		ivar id = azureml://registries/azureml/models/Gleu-Score-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			)
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		@override
		def __init__(
			    self, 
			    *, 
			    threshold = 0.5
			)
		
		
	class azure.ai.evaluation.GroundednessEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Groundedness-Evaluator/versions/4
		
		@overload
		def __call__(
			    self, 
			    *, 
			    context: str, 
			    query: Optional[str] = ..., 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    context: Optional[str] = ..., 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = 3, 
			    **kwargs
			)
		
		
	@experimental
	class azure.ai.evaluation.GroundednessProEvaluator(RaiServiceEvaluatorBase[Union[str, bool]]):
		ivar id = azureml://registries/azureml/models/Groundedness-Pro-Evaluator/versions/1
		
		@overload
		def __call__(
			    self, 
			    *, 
			    context: str, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, bool]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    context: Optional[str] = ..., 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, bool]], Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    threshold: int = 5, 
			    **kwargs: Any
			)
		
		
	@experimental
	class azure.ai.evaluation.HateUnfairnessEvaluator(RaiServiceEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Hate-and-Unfairness-Evaluator/versions/4
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    threshold: int = 3
			)
		
		
	@experimental
	class azure.ai.evaluation.IndirectAttackEvaluator(RaiServiceEvaluatorBase[Union[str, bool]]):
		ivar id = azureml://registries/azureml/models/Indirect-Attack-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, bool]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, bool]], Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject
			)
		
		
	@experimental
	class azure.ai.evaluation.IntentResolutionEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: Union[str, List[dict]], 
			    response: Union[str, List[dict]], 
			    tool_definitions: Optional[Union[dict, List[dict]]] = ...
			) -> Dict[str, Union[str, float]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    **kwargs
			)
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = _DEFAULT_INTENT_RESOLUTION_THRESHOLD, 
			    **kwargs
			)
		
		
	class azure.ai.evaluation.Message(TypedDict):
		key "content": Union[str, List[Dict]]
		key "role": str
		ivar context: NotRequired[Dict[str, Any]]
		
		
	class azure.ai.evaluation.MeteorScoreEvaluator(ABC, Generic[T_EvalValue]):
		ivar id = azureml://registries/azureml/models/Meteor-Score-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			) -> Dict[str, float]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		@override
		def __init__(
			    self, 
			    alpha: float = 0.9, 
			    beta: float = 3.0, 
			    gamma: float = 0.5, 
			    *, 
			    threshold: float = 0.5
			)
		
		
	class azure.ai.evaluation.OpenAIModelConfiguration(TypedDict):
		key "api_key": str
		key "model": str
		ivar base_url: NotRequired[str]
		ivar organization: NotRequired[str]
		ivar type: NotRequired[Literal['openai']]
		
		
	@experimental
	class azure.ai.evaluation.ProtectedMaterialEvaluator(RaiServiceEvaluatorBase[Union[str, bool]]):
		ivar id = azureml://registries/azureml/models/Protected-Material-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, bool]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]
		
		@override
		def __call__(
			    self, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, bool]], Dict[str, Union[float, Dict[str, List[Union[str, bool]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject
			)
		
		
	class azure.ai.evaluation.QAEvaluator(MultiEvaluatorBase[Union[str, float]]):
		ivar id = qa
		
		@overload
		def __call__(
			    self, 
			    *, 
			    context: str, 
			    ground_truth: str, 
			    query: str, 
			    response: str
			)
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    context: str = ..., 
			    ground_truth: str = ..., 
			    query: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, Union[str, float]]
		
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    coherence_threshold: int = 3, 
			    f1_score_threshold: float = 0.5, 
			    fluency_threshold: int = 3, 
			    groundedness_threshold: int = 3, 
			    relevance_threshold: int = 3, 
			    similarity_threshold: int = 3, 
			    **kwargs: Any
			)
		
		
	class azure.ai.evaluation.RelevanceEvaluator(EvaluatorBase[T]):
		ivar id = azureml://registries/azureml/models/Relevance-Evaluator/versions/4
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = 3
			)
		
		
	@experimental
	class azure.ai.evaluation.ResponseCompletenessEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = completeness
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    **kwargs
			)
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold: Optional[float] = _DEFAULT_COMPLETENESS_THRESHOLD, 
			    **kwargs
			)
		
		
	class azure.ai.evaluation.RetrievalEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Retrieval-Evaluator/versions/1
		
		@overload
		def __call__(
			    self, 
			    *, 
			    context: str, 
			    query: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    context: Optional[str] = ..., 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    **kwargs
			) -> :rtype: Dict[str, Union[float, Dict[str, List[str, float]]]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold: float = 3
			) -> Callable
		
		
	class azure.ai.evaluation.RougeScoreEvaluator(ABC, Generic[T_EvalValue]):
		ivar id = azureml://registries/azureml/models/Rouge-Score-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    response: str
			) -> Dict[str, float]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		@override
		def __init__(
			    self, 
			    rouge_type: RougeType, 
			    *, 
			    f1_score_threshold: float = 0.5, 
			    precision_threshold: float = 0.5, 
			    recall_threshold: float = 0.5
			)
		
		
	class azure.ai.evaluation.RougeType(Enum):
		ROUGE_1 = "rouge1"
		ROUGE_2 = "rouge2"
		ROUGE_3 = "rouge3"
		ROUGE_4 = "rouge4"
		ROUGE_5 = "rouge5"
		ROUGE_L = "rougeL"
		
		
	@experimental
	class azure.ai.evaluation.SelfHarmEvaluator(RaiServiceEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Self-Harm-Related-Content-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    threshold: int = 3
			)
		
		
	@experimental
	class azure.ai.evaluation.SexualEvaluator(RaiServiceEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Sexual-Content-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[str, float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    threshold: int = 3
			)
		
		
	class azure.ai.evaluation.SimilarityEvaluator(EvaluatorBase[T]):
		ivar id = similarity
		
		@overload
		def __call__(
			    self, 
			    *, 
			    ground_truth: str, 
			    query: str, 
			    response: str
			) -> Dict[str, float]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    ground_truth: str = ..., 
			    query: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, float]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = 3
			)
		
		
	@experimental
	class azure.ai.evaluation.TaskAdherenceEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: Union[str, List[dict]], 
			    response: Union[str, List[dict]], 
			    tool_definitions: Optional[Union[dict, List[dict]]] = ...
			) -> Dict[str, Union[str, float]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    **kwargs
			)
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = _DEFAULT_TASK_ADHERENCE_SCORE, 
			    **kwargs
			)
		
		
	@experimental
	class azure.ai.evaluation.ToolCallAccuracyEvaluator(PromptyEvaluatorBase[Union[str, float]]):
		ivar id = id
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: Union[str, List[dict]], 
			    response: Union[str, List[dict]] = ..., 
			    tool_calls: Union[dict, List[dict]] = ..., 
			    tool_definitions: Union[dict, List[dict]]
			) -> Dict[str, Union[str, float]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    query: Union[str, List[dict]] = ..., 
			    response: Union[str, List[dict]] = ..., 
			    tool_calls: Union[dict, List[dict]] = ..., 
			    tool_definitions: Union[dict, List[dict]] = ..., 
			    **kwargs
			) -> Dict[str, Union[str, float]]
		
		@override
		def __init__(
			    self, 
			    model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration], 
			    *, 
			    threshold = _DEFAULT_TOOL_CALL_ACCURACY_SCORE, 
			    **kwargs
			)
		
		
	@experimental
	class azure.ai.evaluation.UngroundedAttributesEvaluator(RaiServiceEvaluatorBase[Union[str, bool]]):
		ivar id = ungrounded_attributes
		
		@overload
		def __call__(
			    self, 
			    *, 
			    context: str, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    context: str = ..., 
			    query: str = ..., 
			    response: str = ..., 
			    **kwargs
			) -> Dict[str, Union[str, bool]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject
			)
		
		
	@experimental
	class azure.ai.evaluation.ViolenceEvaluator(RaiServiceEvaluatorBase[Union[str, float]]):
		ivar id = azureml://registries/azureml/models/Violent-Content-Evaluator/versions/3
		
		@overload
		def __call__(
			    self, 
			    *, 
			    query: str, 
			    response: str
			) -> Dict[str, Union[str, float]]
		
		@overload
		def __call__(
			    self, 
			    *, 
			    conversation: Conversation
			) -> Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]
		
		@override
		def __call__(
			    self, 
			    *args, 
			    *, 
			    conversation: Optional[Conversation] = ..., 
			    query: Optional[str] = ..., 
			    response: Optional[str] = ..., 
			    **kwargs
			) -> Union[Dict[str, Union[str, float]], Dict[str, Union[float, Dict[str, List[Union[str, float]]]]]]
		
		@override
		def __init__(
			    self, 
			    credential: TokenCredential, 
			    azure_ai_project: AzureAIProject, 
			    *, 
			    threshold: int = 3
			)
		
		
namespace azure.ai.evaluation.simulator
	
	@experimental
	class azure.ai.evaluation.simulator.AdversarialScenario(Enum):
		ADVERSARIAL_CODE_VULNERABILITY = "adv_code_vuln"
		ADVERSARIAL_CONTENT_GEN_GROUNDED = "adv_content_gen_grounded"
		ADVERSARIAL_CONTENT_GEN_UNGROUNDED = "adv_content_gen_ungrounded"
		ADVERSARIAL_CONTENT_PROTECTED_MATERIAL = "adv_content_protected_material"
		ADVERSARIAL_CONVERSATION = "adv_conversation"
		ADVERSARIAL_QA = "adv_qa"
		ADVERSARIAL_REWRITE = "adv_rewrite"
		ADVERSARIAL_SEARCH = "adv_search"
		ADVERSARIAL_SUMMARIZATION = "adv_summarization"
		ADVERSARIAL_UNGROUNDED_ATTRIBUTES = "adv_isa"
		
		
	@experimental
	class azure.ai.evaluation.simulator.AdversarialScenarioJailbreak(Enum):
		ADVERSARIAL_INDIRECT_JAILBREAK = "adv_xpia"
		
		
	@experimental
	class azure.ai.evaluation.simulator.AdversarialSimulator:
		
		async def __call__(
			    self, 
			    *, 
			    api_call_delay_sec: int = 0, 
			    api_call_retry_limit: int = 3, 
			    api_call_retry_sleep_sec: int = 1, 
			    concurrent_async_task: int = 3, 
			    language: SupportedLanguages = SupportedLanguages.English, 
			    max_conversation_turns: int = 1, 
			    max_simulation_results: int = 3, 
			    randomization_seed: Optional[int] = ..., 
			    randomize_order: bool = True, 
			    scenario: AdversarialScenario, 
			    target: Callable, 
			    **kwargs
			) -> List[Dict[str, Any]]
		
		def __init__(
			    self, 
			    *, 
			    azure_ai_project: AzureAIProject, 
			    credential: TokenCredential
			)
		
		def call_sync(
			    self, 
			    *, 
			    api_call_delay_sec: int, 
			    api_call_retry_limit: int, 
			    api_call_retry_sleep_sec: int, 
			    concurrent_async_task: int, 
			    max_conversation_turns: int, 
			    max_simulation_results: int, 
			    scenario: AdversarialScenario, 
			    target: Callable
			) -> List[Dict[str, Any]]
		
		
	@experimental
	class azure.ai.evaluation.simulator.DirectAttackSimulator:
		
		async def __call__(
			    self, 
			    *, 
			    api_call_delay_sec: int = 0, 
			    api_call_retry_limit: int = 3, 
			    api_call_retry_sleep_sec: int = 1, 
			    concurrent_async_task: int = 3, 
			    max_conversation_turns: int = 1, 
			    max_simulation_results: int = 3, 
			    randomization_seed: Optional[int] = ..., 
			    scenario: AdversarialScenario, 
			    target: Callable
			) -> Dict[str, [List[Dict[str, Any]]]]
		
		def __init__(
			    self, 
			    *, 
			    azure_ai_project: AzureAIProject, 
			    credential: TokenCredential
			)
		
		
	@experimental
	class azure.ai.evaluation.simulator.IndirectAttackSimulator(AdversarialSimulator):
		
		async def __call__(
			    self, 
			    *, 
			    api_call_delay_sec: int = 0, 
			    api_call_retry_limit: int = 3, 
			    api_call_retry_sleep_sec: int = 1, 
			    concurrent_async_task: int = 3, 
			    max_simulation_results: int = 3, 
			    target: Callable, 
			    **kwargs
			) -> List[Dict[str, Any]]
		
		def __init__(
			    self, 
			    *, 
			    azure_ai_project: AzureAIProject, 
			    credential: TokenCredential
			)
		
		def call_sync(
			    self, 
			    *, 
			    api_call_delay_sec: int, 
			    api_call_retry_limit: int, 
			    api_call_retry_sleep_sec: int, 
			    concurrent_async_task: int, 
			    max_conversation_turns: int, 
			    max_simulation_results: int, 
			    scenario: AdversarialScenario, 
			    target: Callable
			) -> List[Dict[str, Any]]
		
		
	@experimental
	class azure.ai.evaluation.simulator.Simulator:
		
		async def __call__(
			    self, 
			    *, 
			    api_call_delay_sec: float = 1, 
			    concurrent_async_tasks: int = 5, 
			    conversation_turns: List[List[Union[str, Dict[str, Any]]]] = [], 
			    max_conversation_turns: int = 5, 
			    num_queries: int = 5, 
			    query_response_generating_prompty: Optional[str] = ..., 
			    query_response_generating_prompty_options: Dict[str, Any] = {}, 
			    target: Callable, 
			    tasks: List[str] = [], 
			    text: str = "", 
			    user_simulator_prompty: Optional[str] = ..., 
			    user_simulator_prompty_options: Dict[str, Any] = {}, 
			    **kwargs
			) -> List[JsonLineChatProtocol]
		
		def __init__(self, model_config: Union[AzureOpenAIModelConfiguration, OpenAIModelConfiguration])
		
		
	class azure.ai.evaluation.simulator.SupportedLanguages(Enum):
		English = "en"
		French = "fr"
		German = "de"
		Italian = "it"
		Japanese = "ja"
		Portuguese = "pt"
		SimplifiedChinese = "zh-cn"
		Spanish = "es"
		
		